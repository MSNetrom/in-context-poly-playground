model: &std_gpt2
    type: gpt2
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2

model: &std_llama
    type: llama
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2

model: &knn_3
    type: knn
    n_neighbors: 3

model: &least_squares
    type: least squares

model: &xgboost
    type: xgboost

model: &mambafirstgpt2_no_pos_embed
    type: mambafirstgpt2
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: False
    no_attention: False
    num_mamba_layers: 1
    custom_attn_func: "vanilla"

model: &mambafirstgpt2_pos_embed
    type: mambafirstgpt2
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: True
    no_attention: False
    num_mamba_layers: 4
    custom_attn_func: "vanilla"

model: &mambaonly_pos_embed
    type: mambaonly
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: True
    no_attention: True
    num_mamba_layers: 10
    custom_attn_func: "vanilla"

model: &mambaonly_no_pos_embed
    type: mambaonly
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: False
    no_attention: True
    num_mamba_layers: 5
    custom_attn_func: "vanilla"


model: &mambaformer_classic
    type: mambaformer_classic
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: False
    no_attention: False
    num_mamba_layers: 1
    num_mamba_instances: 2
    custom_attn_func: "vanilla"

model: &mod_transformer
    type: mod_transformer
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2
    want_pos_embeddings: True
    no_attention: False
    custom_attn_func: "relu"
