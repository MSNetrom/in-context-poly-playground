model: &std_gpt2
    type: gpt2
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2

model: &std_llama
    type: llama
    n_positions: 101
    n_layer: 3
    n_embd: 64
    n_head: 2

model: &lora_std_gpt2
    type: lora
    base_model: *std_gpt2
    lora_config:
        r: 16
        lora_alpha: 16
        bias: none
        fan_in_fan_out: True
        lora_dropout: 0.0
        target_modules: ["attn.c_attn", "mlp.c_fc", "mlp.c_proj"]

model: &lora_std_llama
    type: lora
    base_model: *std_llama
    lora_config:
        r: 16
        lora_alpha: 16
        bias: none
        fan_in_fan_out: False
        lora_dropout: 0.0
        target_modules: ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj"]

model: &knn_3
    type: knn
    n_neighbors: 3

model: &least_squares
    type: least squares

model: &averaging
    type: averaging

model: &lasso
    type: lasso
    alpha: 0.1

model: &grad_mlp
    type: grad mlp
    model_class_name: mlp
    model_class_args:
        dimensions: [10, 256, 1]

model: &d_tree
    type: decision tree

model: &xgboost
    type: xgboost

model: &zero
    type: zero
